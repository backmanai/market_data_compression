{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Library Comparison: Polars vs Pandas vs Dask\n",
    "\n",
    "**Real-world performance test on event extraction from market data**\n",
    "\n",
    "This notebook compares three popular DataFrame libraries on a real data engineering task:\n",
    "extracting trading events from market order snapshots.\n",
    "\n",
    "## The Task\n",
    "\n",
    "Event extraction requires:\n",
    "1. **Reading Parquet files** (I/O performance)\n",
    "2. **Filtering data** by conditions\n",
    "3. **Joining snapshots** on order_id\n",
    "4. **Detecting changes** (disappeared orders, volume changes, new orders)\n",
    "\n",
    "## The Libraries\n",
    "\n",
    "- **Polars**: Modern, multi-threaded, built on Apache Arrow\n",
    "- **Pandas**: Classic, single-threaded, most widely used\n",
    "- **Dask**: Parallel/distributed computing, pandas-compatible API\n",
    "\n",
    "Let's see which one performs best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration:\n",
      "   Data path: /Users/fred/Projects/Gaming/Eve Online/eve_market_data/data/orders\n",
      "   Testing 3 snapshot pairs\n",
      "   Backends: polars, pandas, dask\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Data location\n",
    "EVE_DATA_PATH = Path(\"/Users/fred/Projects/Gaming/Eve Online/eve_market_data/data/orders\")\n",
    "\n",
    "# How many snapshot pairs to test (more = longer test, more accurate)\n",
    "NUM_SNAPSHOT_PAIRS = 3  # Use 3 for quick test, 11 for full hour\n",
    "\n",
    "# Backends to test\n",
    "BACKENDS = ['polars', 'pandas', 'dask']\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "print(f\"   Data path: {EVE_DATA_PATH}\")\n",
    "print(f\"   Testing {NUM_SNAPSHOT_PAIRS} snapshot pairs\")\n",
    "print(f\"   Backends: {', '.join(BACKENDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import libraries and load snapshot files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 12 snapshot files\n",
      "\n",
      "‚è±Ô∏è  Time range:\n",
      "   Start: region_10000002_2025-10-22T07-00-00+00-00.parquet\n",
      "   End:   region_10000002_2025-10-22T07-55-00+00-00.parquet\n",
      "\n",
      "üîç Will test with 3 snapshot pairs\n"
     ]
    }
   ],
   "source": [
    "# Find snapshot files\n",
    "snapshot_files = sorted(EVE_DATA_PATH.glob(\"region_10000002_2025-10-22T07-*.parquet\"))\n",
    "snapshot_files += sorted(EVE_DATA_PATH.glob(\"region_10000002_2025-10-22T08-*.parquet\"))[:3]\n",
    "snapshot_files = snapshot_files[:12]\n",
    "\n",
    "print(f\"üìÇ Found {len(snapshot_files)} snapshot files\")\n",
    "print(f\"\\n‚è±Ô∏è  Time range:\")\n",
    "print(f\"   Start: {snapshot_files[0].name}\")\n",
    "print(f\"   End:   {snapshot_files[-1].name}\")\n",
    "\n",
    "# Create snapshot pairs for testing\n",
    "snapshot_pairs = [\n",
    "    (snapshot_files[i], snapshot_files[i+1]) \n",
    "    for i in range(min(NUM_SNAPSHOT_PAIRS, len(snapshot_files)-1))\n",
    "]\n",
    "\n",
    "print(f\"\\nüîç Will test with {len(snapshot_pairs)} snapshot pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Performance Test: Real Event Extraction with Initialization\n",
    "\n",
    "We'll run the exact same event detection logic with three different DataFrame backends.\n",
    "\n",
    "### Complete Event Extraction Process:\n",
    "\n",
    "**Step 1: Initialization** (create baseline state)\n",
    "- Read first snapshot (snapshot \"0\")\n",
    "- Generate ORDER_OPENED events for all existing orders\n",
    "- This establishes the baseline state, making the event log self-contained\n",
    "\n",
    "**Step 2: Delta Event Detection** (process subsequent snapshots)\n",
    "1. Read two consecutive snapshots (Parquet files)\n",
    "2. Identify disappeared orders (‚Üí TRADE or CANCELLED events)\n",
    "3. Detect volume reductions (‚Üí TRADE events)\n",
    "4. Find new orders (‚Üí ORDER_OPENED events)\n",
    "5. Spot price changes (‚Üí PRICE_CHANGED events)\n",
    "\n",
    "This is a real-world workload combining I/O, joins, filters, and transformations.\n",
    "\n",
    "**Why initialization matters**: With the baseline events from snapshot \"0\", we can reconstruct the complete order book state at any timestamp using only the event log - no anchor snapshots needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Comparing OPTIMIZED event extraction performance\n",
      "   Testing with 3 snapshot pairs\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìä POLARS (vectorized, multi-threaded)\n",
      "üå± Initializing event log from snapshot: 375,338 orders\n",
      "   Time: 1.142 seconds\n",
      "   Events: 378,250 (375,338 init + 2,912 delta)\n",
      "   Avg per interval: 0.381s\n",
      "\n",
      "üìä PANDAS (vectorized where possible, single-threaded)\n",
      "üå± Initializing event log from snapshot: 375,338 orders\n",
      "   Time: 2.257 seconds\n",
      "   Events: 378,250 (375,338 init + 2,912 delta)\n",
      "   Avg per interval: 0.752s\n",
      "\n",
      "üìä DASK (parallel batch processing across CPU cores)\n",
      "üå± Initializing event log from snapshot: 375,338 orders\n",
      "   Time: 2.112 seconds\n",
      "   Events: 378,250 (375,338 init + 2,912 delta)\n",
      "   Avg per interval: 0.704s\n",
      "   (Processing 3 pairs in parallel!)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"üîç Comparing OPTIMIZED event extraction performance\")\n",
    "print(f\"   Testing with {len(snapshot_pairs)} snapshot pairs\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Import optimized detectors\n",
    "from src.event_extractor.event_detector_polars import PolarsEventDetector\n",
    "from src.event_extractor.event_detector_pandas import PandasEventDetector\n",
    "from src.event_extractor.event_detector_dask import DaskEventDetector\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Test 1: POLARS (optimized - vectorized operations)\n",
    "print(\"\\nüìä POLARS (vectorized, multi-threaded)\")\n",
    "detector_polars = PolarsEventDetector()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Step 1: Initialize from first snapshot (baseline state)\n",
    "timestamp_0 = datetime.strptime(snapshot_files[0].stem.split('_')[-1].replace('+00-00', ''), \"%Y-%m-%dT%H-%M-%S\")\n",
    "init_events = detector_polars.initialize_from_snapshot(snapshot_files[0], timestamp_0)\n",
    "total_events = len(init_events)\n",
    "\n",
    "# Step 2: Process delta events from snapshot pairs\n",
    "for prev_file, curr_file in snapshot_pairs:\n",
    "    timestamp_str = curr_file.stem.split('_')[-1].replace('+00-00', '')\n",
    "    timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H-%M-%S\")\n",
    "    events = detector_polars.detect_events(prev_file, curr_file, timestamp)\n",
    "    total_events += len(events)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "results['polars'] = {\n",
    "    'time': elapsed,\n",
    "    'events': total_events,\n",
    "    'init_events': len(init_events),\n",
    "    'delta_events': total_events - len(init_events),\n",
    "    'avg_per_interval': elapsed / len(snapshot_pairs)\n",
    "}\n",
    "\n",
    "print(f\"   Time: {elapsed:.3f} seconds\")\n",
    "print(f\"   Events: {total_events:,} ({len(init_events):,} init + {total_events - len(init_events):,} delta)\")\n",
    "print(f\"   Avg per interval: {elapsed/len(snapshot_pairs):.3f}s\")\n",
    "\n",
    "# Test 2: PANDAS (optimized - vectorized groupby, itertuples)\n",
    "print(\"\\nüìä PANDAS (vectorized where possible, single-threaded)\")\n",
    "detector_pandas = PandasEventDetector()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Step 1: Initialize from first snapshot\n",
    "init_events = detector_pandas.initialize_from_snapshot(snapshot_files[0], timestamp_0)\n",
    "total_events = len(init_events)\n",
    "\n",
    "# Step 2: Process delta events\n",
    "for prev_file, curr_file in snapshot_pairs:\n",
    "    timestamp_str = curr_file.stem.split('_')[-1].replace('+00-00', '')\n",
    "    timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H-%M-%S\")\n",
    "    events = detector_pandas.detect_events(prev_file, curr_file, timestamp)\n",
    "    total_events += len(events)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "results['pandas'] = {\n",
    "    'time': elapsed,\n",
    "    'events': total_events,\n",
    "    'init_events': len(init_events),\n",
    "    'delta_events': total_events - len(init_events),\n",
    "    'avg_per_interval': elapsed / len(snapshot_pairs)\n",
    "}\n",
    "\n",
    "print(f\"   Time: {elapsed:.3f} seconds\")\n",
    "print(f\"   Events: {total_events:,} ({len(init_events):,} init + {total_events - len(init_events):,} delta)\")\n",
    "print(f\"   Avg per interval: {elapsed/len(snapshot_pairs):.3f}s\")\n",
    "\n",
    "# Test 3: DASK (batch processing - TRUE parallelism)\n",
    "print(\"\\nüìä DASK (parallel batch processing across CPU cores)\")\n",
    "detector_dask = DaskEventDetector(n_workers=4)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Step 1: Initialize from first snapshot\n",
    "init_events = detector_dask.initialize_from_snapshot(snapshot_files[0], timestamp_0)\n",
    "\n",
    "# Step 2: Prepare batch data for parallel processing\n",
    "batch_pairs = [\n",
    "    (prev, curr, datetime.strptime(curr.stem.split('_')[-1].replace('+00-00', ''), \"%Y-%m-%dT%H-%M-%S\"))\n",
    "    for prev, curr in snapshot_pairs\n",
    "]\n",
    "\n",
    "# Process all pairs in parallel\n",
    "delta_events = detector_dask.detect_events_batch(batch_pairs)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "total_events = len(init_events) + len(delta_events)\n",
    "\n",
    "results['dask'] = {\n",
    "    'time': elapsed,\n",
    "    'events': total_events,\n",
    "    'init_events': len(init_events),\n",
    "    'delta_events': len(delta_events),\n",
    "    'avg_per_interval': elapsed / len(snapshot_pairs)\n",
    "}\n",
    "\n",
    "print(f\"   Time: {elapsed:.3f} seconds\")\n",
    "print(f\"   Events: {total_events:,} ({len(init_events):,} init + {len(delta_events):,} delta)\")\n",
    "print(f\"   Avg per interval: {elapsed/len(snapshot_pairs):.3f}s\")\n",
    "print(f\"   (Processing {len(snapshot_pairs)} pairs in parallel!)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° SPEEDUP COMPARISON (vs Pandas):\n",
      "======================================================================\n",
      "   Polars    1.98√ó faster than Pandas\n",
      "   Dask      1.07√ó faster than Pandas\n",
      "   Pandas   baseline\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìä DETAILED BREAKDOWN (fastest ‚Üí slowest):\n",
      "======================================================================\n",
      "Rank   Backend      Total Time    Avg/Interval     Events\n",
      "----------------------------------------------------------------------\n",
      "ü•á 1. Polars          1.142s         0.381s    378,250\n",
      "ü•à 2. Dask            2.112s         0.704s    378,250\n",
      "ü•â 3. Pandas          2.257s         0.752s    378,250\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sort backends by performance (fastest to slowest)\n",
    "sorted_backends = sorted(BACKENDS, key=lambda b: results[b]['time'])\n",
    "winner = sorted_backends[0]\n",
    "pandas_time = results['pandas']['time']\n",
    "winner_time = results[winner]['time']\n",
    "\n",
    "print(\"\\n‚ö° SPEEDUP COMPARISON (vs Pandas):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for backend in sorted_backends:\n",
    "    if backend != 'pandas':\n",
    "        speedup = pandas_time / results[backend]['time']\n",
    "        print(f\"   {backend.capitalize():8} {speedup:5.2f}√ó faster than Pandas\")\n",
    "    else:\n",
    "        print(f\"   {backend.capitalize():8} baseline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Show detailed breakdown - ordered by performance\n",
    "print(\"\\nüìä DETAILED BREAKDOWN (fastest ‚Üí slowest):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Rank':<6} {'Backend':<10} {'Total Time':>12} {'Avg/Interval':>15} {'Events':>10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for rank, backend in enumerate(sorted_backends, 1):\n",
    "    r = results[backend]\n",
    "    medal = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\"\n",
    "    print(f\"{medal} {rank}. {backend.capitalize():<10} {r['time']:>10.3f}s {r['avg_per_interval']:>13.3f}s {r['events']:>10,}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
