{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Data Compression: from json to delta events\n",
    "\n",
    "This notebook demonstrates how bulky market data snapshots of open orders in the Eve Online universe can be handled by applying multiple steps of compression.\n",
    "\n",
    "1. **Stage 1: JSON → Parquet** Columnar storage with Snappy compression\n",
    "2. **Stage 2: Event Sourcing**: Storing deltas instead of full snapshots\n",
    "\n",
    "\n",
    "## Problem description\n",
    "\n",
    "Eve Online is a massive multiplayer game taking place in space. It has a unique in-game economy in which almost every item like ships, weapons, fuel etc is produced by players. At any given time approximately 30'000 players are online.\n",
    "\n",
    "The goal is to obeserve and analysis the in-game trading market. For this eve has public REST endpoints which reveal some information.\n",
    "\n",
    "By observing the open market orders (buy and sell side) we can analyze the volume traded and the payed prices, data which is not public, yet very useful for market trading success.\n",
    "\n",
    "The open order refresh every 5 minutes. The most active trading region is \"The Forge\" with approx. 400k open orders at all times. This demonstration focuses on this particular region.\n",
    "\n",
    "A raw json snapshot is 100MB. at 288 snapshots per day the data volume accumulating can't be handled realistically. \n",
    "Even after converting json to parquet, one snapshot is still 8MB and produces a lot of data over time if we want to keep the order book history available for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and initialize our data pipeline components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import polars as pl\n",
    "\n",
    "# Import our utilities\n",
    "from src.event_extractor.event_detector_polars import PolarsEventDetector\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stage 1: JSON → Parquet Compression\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "**Before proceeding**, run the data fetching script to prepare the demonstration files:\n",
    "\n",
    "```bash\n",
    "python src/utils/fetch_and_convert.py\n",
    "```\n",
    "\n",
    "This script will:\n",
    "1. Fetch ~400k market orders from the EVE Online ESI API (~60 seconds)\n",
    "2. Save the raw data as JSON\n",
    "3. Convert to Parquet format with Snappy compression\n",
    "4. Store both files in `./data/demo/`\n",
    "---\n",
    "\n",
    "### Why Parquet over JSON?\n",
    "\n",
    "**Parquet** is a columnar storage format optimized for analytics that provides massive compression advantages:\n",
    "\n",
    "#### Columnar Layout\n",
    "- Stores data **by column** instead of row-by-row\n",
    "- All values for one field are stored together (e.g., all `order_id` values in sequence)\n",
    "- Perfect for analytics where you typically query specific columns, not entire rows\n",
    "\n",
    "#### Built-in Compression Layers\n",
    "\n",
    "**1. Dictionary Encoding:**\n",
    "- Replaces repeated values with small integer references\n",
    "- Example: If the region_id appears 400k times, store it once + 400k tiny references\n",
    "- Extremely effective for fields with limited unique values (type IDs, locations, buy/sell flags)\n",
    "\n",
    "**2. Snappy Compression:**\n",
    "- Fast, general-purpose compression algorithm by google, applied after encoding\n",
    "- Prioritizes speed over maximum compression (still achieves 2-4× reduction)\n",
    "- Unlike gzip/zstd, optimized for decompression speed in analytics workloads\n",
    "\n",
    "**3. Type Efficiency:**\n",
    "- Binary encoding vs. text representation\n",
    "- Example: The number `123456789` takes:\n",
    "  - **9 bytes** in JSON (9 ASCII characters)\n",
    "  - **4 bytes** in Parquet (32-bit integer)\n",
    "- Timestamps, floats, booleans all stored in compact binary form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading demonstration files:\n",
      "   JSON:    market_orders.json\n",
      "   Parquet: market_orders.parquet\n",
      "\n",
      "Loaded 402,300 market orders\n",
      "\n",
      "Sample order structure:\n",
      "{\n",
      "  \"duration\": 90,\n",
      "  \"is_buy_order\": false,\n",
      "  \"issued\": \"2025-11-15T20:13:29Z\",\n",
      "  \"location_id\": 60003760,\n",
      "  \"min_volume\": 1,\n",
      "  \"order_id\": 7186939904,\n",
      "  \"price\": 232700.0,\n",
      "  \"range\": \"region\",\n",
      "  \"system_id\": 30000142,\n",
      "  \"type_id\": 570,\n",
      "  \"volume_remain\": 3,\n",
      "  \"volume_total\": 5\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load the expected demo files\n",
    "json_path = Path(\"./data/demo/market_orders.json\")\n",
    "parquet_path = Path(\"./data/demo/market_orders.parquet\")\n",
    "\n",
    "# Check if files exist\n",
    "if not json_path.exists() or not parquet_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Demo data files not found in ./data/demo/\\n\"\n",
    "        \"Please run: python src/utils/fetch_and_convert.py\"\n",
    "    )\n",
    "\n",
    "print(f\"Loading demonstration files:\")\n",
    "print(f\"   JSON:    {json_path.name}\")\n",
    "print(f\"   Parquet: {parquet_path.name}\")\n",
    "\n",
    "# Load JSON to get order count\n",
    "with open(json_path, 'r') as f:\n",
    "    orders = json.load(f)\n",
    "\n",
    "print(f\"\\nLoaded {len(orders):,} market orders\")\n",
    "print(f\"\\nSample order structure:\")\n",
    "print(json.dumps(orders[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze JSON file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file size: 120.93 MB (126,801,130 bytes)\n",
      "Average bytes per order: 315.2 bytes\n"
     ]
    }
   ],
   "source": [
    "# Measure JSON file size\n",
    "json_size_bytes = json_path.stat().st_size\n",
    "json_size_mb = json_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"JSON file size: {json_size_mb:.2f} MB ({json_size_bytes:,} bytes)\")\n",
    "print(f\"Average bytes per order: {json_size_bytes / len(orders):.1f} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Parquet file size and compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression Results (Stage 1: JSON → Parquet)\n",
      "============================================================\n",
      "Raw JSON:          120.93 MB\n",
      "Parquet:             9.52 MB\n",
      "Compression:         12.7× reduction\n",
      "Avg per order:       24.8 bytes (Parquet)\n",
      "                 vs 315.2 bytes (JSON)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Measure Parquet file size\n",
    "parquet_size_bytes = parquet_path.stat().st_size\n",
    "parquet_size_mb = parquet_size_bytes / (1024 * 1024)\n",
    "\n",
    "# Calculate compression ratio\n",
    "compression_ratio = json_size_bytes / parquet_size_bytes\n",
    "\n",
    "print(f\"Compression Results (Stage 1: JSON → Parquet)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Raw JSON:        {json_size_mb:8.2f} MB\")\n",
    "print(f\"Parquet:         {parquet_size_mb:8.2f} MB\")\n",
    "print(f\"Compression:     {compression_ratio:8.1f}× reduction\")\n",
    "print(f\"Avg per order:   {parquet_size_bytes / len(orders):8.1f} bytes (Parquet)\")\n",
    "print(f\"                 vs {json_size_bytes / len(orders):.1f} bytes (JSON)\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "By converting the results to parquet we could retain a couple of days of snapshot data (2.7GB) on the server. However space will quickly run-out on the local server and either require payed S3 storage somewhere in the cloud. Or we would have to restrict the history we keep.\n",
    "\n",
    "Further compression is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stage 2: Event Extraction\n",
    "\n",
    "Instead of storing full snapshots every 5 minutes (2.7 GB/day), we extract **events** (changes) between snapshots.\n",
    "\n",
    "With this approach we can reconstruct the order book at any point in time. The events themselves hold valuable market dynamics insights as well.\n",
    "\n",
    "The possible events are:\n",
    "\n",
    "```python\n",
    "TRADE = \"trade\"                    # Volume was traded (partial fill: volume reduced)\n",
    "ORDER_OPENED = \"order_opened\"      # New order appeared\n",
    "ORDER_CLOSED = \"order_closed\"      # Order with volume=0 disappeared (fully filled)\n",
    "ORDER_CANCELLED = \"order_cancelled\" # Order manually cancelled (disappeared, not scheduled to expire)\n",
    "ORDER_EXPIRED = \"order_expired\"    # Order expired naturally (scheduled expiration time reached)\n",
    "PRICE_CHANGED = \"price_changed\"    # Order price was modified\n",
    "```\n",
    "\n",
    "The trade events will give insights both on the buy and sell side for the executed prices, the volume moved etc.\n",
    "The price change events will reveal the trader competition as the traders compete to provide the best price.\n",
    "\n",
    "\n",
    "In order to reconstruct the data based on events, we need a baseline to start from. \n",
    "\n",
    "\n",
    "### The Two-Phase Approach:\n",
    "\n",
    "#### Phase 1: Initialization\n",
    "\n",
    "Establish the baseline. In this demonstration it is the first available snapshot. In production there will be a daily baseline by closing all orders at the end of the day and re-open all trades the next day.\n",
    "\n",
    "Process:\n",
    "\n",
    "- Take the **first snapshot** and create one ORDER_OPENED event per order\n",
    "- This establishes the baseline state\n",
    "- Size: ~1 event per order in the market\n",
    "\n",
    "#### Phase 2: Delta Extraction  \n",
    "\n",
    "- Compare each **subsequent snapshot** to the previous one\n",
    "- Only extract events for orders that **changed**\n",
    "- Typical changes: trades (volume reduction), cancellations, new orders, price changes\n",
    "- Size: expected ~<1% of orders change between snapshots\n",
    "\n",
    "This is where the massive compression happens without information loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 consecutive snapshots:\n",
      "   T0: region_10000002_2025-10-22T07-00-00+00-00.parquet\n",
      "   T1: region_10000002_2025-10-22T07-05-00+00-00.parquet\n",
      "\n",
      "Snapshot sizes:\n",
      "   T0: 8.52 MB\n",
      "   T1: 8.52 MB\n",
      "   Total: 17.05 MB for 2 snapshots\n"
     ]
    }
   ],
   "source": [
    "# Load two consecutive snapshots for demonstration (5 minutes apart)\n",
    "snapshot_t0 = Path(\"./data/snapshots/region_10000002_2025-10-22T07-00-00+00-00.parquet\")\n",
    "snapshot_t1 = Path(\"./data/snapshots/region_10000002_2025-10-22T07-05-00+00-00.parquet\")\n",
    "\n",
    "# Check if files exist\n",
    "if not snapshot_t0.exists() or not snapshot_t1.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Snapshot files not found in ./data/snapshots/\\n\"\n",
    "        \"These should be included in the project repository.\"\n",
    "    )\n",
    "\n",
    "print(f\"Loaded 2 consecutive snapshots:\")\n",
    "print(f\"   T0: {snapshot_t0.name}\")\n",
    "print(f\"   T1: {snapshot_t1.name}\")\n",
    "\n",
    "# Measure snapshot sizes\n",
    "t0_size = snapshot_t0.stat().st_size / (1024 * 1024)\n",
    "t1_size = snapshot_t1.stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nSnapshot sizes:\")\n",
    "print(f\"   T0: {t0_size:.2f} MB\")\n",
    "print(f\"   T1: {t1_size:.2f} MB\")\n",
    "print(f\"   Total: {t0_size + t1_size:.2f} MB for 2 snapshots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Initialization - Extract baseline events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Initialization\n",
      "   Processing first snapshot at 2025-10-22 07:00:00\n",
      "Initializing event log from snapshot: 375,338 orders\n",
      "\n",
      "Initialization Results:\n",
      "   Orders in T0:        375,338\n",
      "   ORDER_OPENED events: 375,338\n",
      "   Ratio:               1 event per order (baseline)\n"
     ]
    }
   ],
   "source": [
    "# Initialize detector\n",
    "detector = PolarsEventDetector()\n",
    "\n",
    "# Phase 1: Initialize from first snapshot\n",
    "# This creates 1 ORDER_OPENED event per order\n",
    "timestamp_t0_str = snapshot_t0.stem.split('_')[-1].replace('+00-00', '')\n",
    "timestamp_t0 = datetime.strptime(timestamp_t0_str, \"%Y-%m-%dT%H-%M-%S\")\n",
    "\n",
    "print(\"Phase 1: Initialization\")\n",
    "print(f\"   Processing first snapshot at {timestamp_t0}\")\n",
    "\n",
    "init_events_df = detector.initialize_from_snapshot(snapshot_t0, timestamp_t0)\n",
    "\n",
    "print(f\"\\nInitialization Results:\")\n",
    "print(f\"   Orders in T0:        {pl.read_parquet(snapshot_t0).height:,}\")\n",
    "print(f\"   ORDER_OPENED events: {init_events_df.height:,}\")\n",
    "print(f\"   Ratio:               1 event per order (baseline)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Delta Extraction - Detect changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2: Delta Extraction\n",
      "   Comparing T0 → T1 (5 minute interval)\n",
      "\n",
      "Delta Extraction Results:\n",
      "   Orders in T1:        375,320\n",
      "   Delta events:        1,023\n",
      "   Change rate:         0.3% of orders changed\n",
      "\n",
      "Delta event types:\n",
      "   trade                514\n",
      "   price_changed        221\n",
      "   order_cancelled      149\n",
      "   order_opened         135\n",
      "   order_closed         4\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Extract delta events between T0 and T1\n",
    "timestamp_t1_str = snapshot_t1.stem.split('_')[-1].replace('+00-00', '')\n",
    "timestamp_t1 = datetime.strptime(timestamp_t1_str, \"%Y-%m-%dT%H-%M-%S\")\n",
    "\n",
    "print(\"Phase 2: Delta Extraction\")\n",
    "print(f\"   Comparing T0 → T1 (5 minute interval)\")\n",
    "\n",
    "delta_events_df = detector.detect_events(snapshot_t0, snapshot_t1, timestamp_t1)\n",
    "\n",
    "print(f\"\\nDelta Extraction Results:\")\n",
    "print(f\"   Orders in T1:        {pl.read_parquet(snapshot_t1).height:,}\")\n",
    "print(f\"   Delta events:        {delta_events_df.height:,}\")\n",
    "print(f\"   Change rate:         {(delta_events_df.height / pl.read_parquet(snapshot_t1).height) * 100:.1f}% of orders changed\")\n",
    "\n",
    "# Show event type breakdown\n",
    "event_type_counts = delta_events_df.group_by('event_type').agg(\n",
    "    pl.len().alias('count')\n",
    ").sort('count', descending=True)\n",
    "\n",
    "print(f\"\\nDelta event types:\")\n",
    "for row in event_type_counts.iter_rows(named=True):\n",
    "    print(f\"   {row['event_type']:20} {row['count']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine events and measure compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 376,361 events to demo_events.parquet (9.15 MB)\n",
      "\n",
      "Compression Results (Stage 2: Snapshots → Events)\n",
      "======================================================================\n",
      "Two snapshots (T0 + T1):         17.05 MB\n",
      "Events (init + delta):             9.15 MB\n",
      "   └─ Initialization events:   375,338\n",
      "   └─ Delta events:            1,023\n",
      "   └─ Total events:            376,361\n",
      "\n",
      "Compression ratio:                  1.9× reduction\n",
      "Bytes per event:                   25.5 bytes\n",
      "======================================================================\n",
      "\n",
      "Key Insight:\n",
      "   After initialization, only 1,023 events needed\n",
      "   vs 375,320 orders in full snapshot\n",
      "   = 0.3% data to store!\n"
     ]
    }
   ],
   "source": [
    "# Combine initialization + delta events\n",
    "all_events_df = pl.concat([init_events_df, delta_events_df])\n",
    "\n",
    "# Store events as Parquet\n",
    "events_dir = Path(\"./data/demo\")\n",
    "events_dir.mkdir(parents=True, exist_ok=True)\n",
    "events_path = events_dir / \"demo_events.parquet\"\n",
    "\n",
    "all_events_df.write_parquet(events_path, compression=\"snappy\")\n",
    "\n",
    "# Measure compression\n",
    "events_size_bytes = events_path.stat().st_size\n",
    "events_size_mb = events_size_bytes / (1024 * 1024)\n",
    "total_snapshot_size_mb = t0_size + t1_size\n",
    "\n",
    "snapshot_to_events_ratio = (total_snapshot_size_mb * 1024 * 1024) / events_size_bytes\n",
    "\n",
    "print(f\"Stored {all_events_df.height:,} events to {events_path.name} ({events_size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nCompression Results (Stage 2: Snapshots → Events)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Two snapshots (T0 + T1):      {total_snapshot_size_mb:8.2f} MB\")\n",
    "print(f\"Events (init + delta):         {events_size_mb:8.2f} MB\")\n",
    "print(f\"   └─ Initialization events:   {init_events_df.height:,}\")\n",
    "print(f\"   └─ Delta events:            {delta_events_df.height:,}\")\n",
    "print(f\"   └─ Total events:            {all_events_df.height:,}\")\n",
    "print(f\"\\nCompression ratio:             {snapshot_to_events_ratio:8.1f}× reduction\")\n",
    "print(f\"Bytes per event:               {events_size_bytes / all_events_df.height:8.1f} bytes\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nKey Insight:\")\n",
    "print(f\"   After initialization, only {delta_events_df.height:,} events needed\")\n",
    "print(f\"   vs {pl.read_parquet(snapshot_t1).height:,} orders in full snapshot\")\n",
    "print(f\"   = {(delta_events_df.height / pl.read_parquet(snapshot_t1).height) * 100:.1f}% data to store!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the production setup we are interested in daily summaries. The end of day processing will calculate all market statistics we are currently interested in. \n",
    "\n",
    "The order book information (baseline + events) of the day will however be kept should at some point the methodology change or other analyses be needed.\n",
    "\n",
    "The true compression ratio should therefore be analyzed based on a full day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Daily Projection\n",
    "\n",
    "Let's extrapolate our 2-snapshot demonstration to a full day (288 snapshots, every 5 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate daily storage requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Event Volume:\n",
      "======================================================================\n",
      "Initialization (once/day):                                375,338 events\n",
      "Delta events (287 intervals at approx. 1023 each):       293,601 events\n",
      "Total daily events:                                       668,939 events\n",
      "======================================================================\n",
      "\n",
      "Daily Storage Comparison:\n",
      "======================================================================\n",
      "Naive (keep all snapshots):     2454.57 MB/day\n",
      "Event sourcing (Parquet):         16.26 MB/day\n",
      "Compression ratio:                150.9×\n",
      "======================================================================\n",
      "\n",
      "Key Insight:\n",
      "   After initialization, only 1,023 events per 5-min interval\n",
      "   = 0.27% of baseline data per snapshot\n",
      "   = Storing changes is 150.9× more efficient!\n"
     ]
    }
   ],
   "source": [
    "# Based on our 2-snapshot sample\n",
    "init_events = init_events_df.height\n",
    "delta_events = delta_events_df.height\n",
    "\n",
    "# Project for full day (288 snapshots every 5 minutes)\n",
    "# Day 1: 1 initialization + 287 delta intervals\n",
    "daily_init_events = init_events\n",
    "daily_delta_events = delta_events * 287\n",
    "\n",
    "total_daily_events = daily_init_events + daily_delta_events\n",
    "\n",
    "print(f\"Daily Event Volume:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Initialization (once/day):                             {daily_init_events:>10,} events\")\n",
    "print(f\"Delta events (287 intervals at approx. {delta_events} each):    {daily_delta_events:>10,} events\")\n",
    "print(f\"Total daily events:                                    {total_daily_events:>10,} events\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Calculate storage\n",
    "bytes_per_event = events_size_bytes / all_events_df.height\n",
    "daily_event_storage_mb = (total_daily_events * bytes_per_event) / (1024 * 1024)\n",
    "\n",
    "# Compare with naive snapshot storage\n",
    "naive_daily_snapshots_mb = (t0_size + t1_size) / 2 * 288  # 288 snapshots per day\n",
    "compression_ratio_daily = naive_daily_snapshots_mb / daily_event_storage_mb\n",
    "\n",
    "print(f\"\\nDaily Storage Comparison:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Naive (keep all snapshots):    {naive_daily_snapshots_mb:>8.2f} MB/day\")\n",
    "print(f\"Event sourcing (Parquet):      {daily_event_storage_mb:>8.2f} MB/day\")\n",
    "print(f\"Compression ratio:             {compression_ratio_daily:>8.1f}×\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nKey Insight:\")\n",
    "print(f\"   After initialization, only {delta_events:,} events per 5-min interval\")\n",
    "print(f\"   = {(delta_events / init_events) * 100:.2f}% of baseline data per snapshot\")\n",
    "print(f\"   = Storing changes is {compression_ratio_daily:.1f}× more efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Let's review what we demonstrated with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPRESSION DEMONSTRATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Stage 1: JSON → Parquet (Columnar Storage)\n",
      "   Raw JSON:                  120.93 MB\n",
      "   Parquet (Snappy):          9.52 MB\n",
      "   Compression:               12.7×\n",
      "\n",
      "Stage 2: Event Sourcing (2 Snapshots)\n",
      "   Two snapshots (T0 + T1):   17.05 MB\n",
      "   Events (init + delta):     9.15 MB\n",
      "   Compression:               1.9×\n",
      "\n",
      "   Events breakdown:\n",
      "      Initialization:         375,338 events (baseline)\n",
      "      Delta (5 min):          1,023 events (0.27% changed)\n",
      "\n",
      "Daily Projection (288 Snapshots)\n",
      "   288 JSON files:            34,827 MB/day\n",
      "   288 Parquet files:         2,455 MB/day (14.2× vs JSON)\n",
      "   Event sourcing:            16 MB/day (150.9× vs Parquet)\n",
      "\n",
      "   Total compression:         2141.7× (JSON 28965MB → Events 15MB)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"COMPRESSION DEMONSTRATION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nStage 1: JSON → Parquet (Columnar Storage)\")\n",
    "print(f\"   Raw JSON:                  {json_size_mb:.2f} MB\")\n",
    "print(f\"   Parquet (Snappy):          {parquet_size_mb:.2f} MB\")\n",
    "print(f\"   Compression:               {compression_ratio:.1f}×\")\n",
    "\n",
    "print(f\"\\nStage 2: Event Sourcing (2 Snapshots)\")\n",
    "print(f\"   Two snapshots (T0 + T1):   {total_snapshot_size_mb:.2f} MB\")\n",
    "print(f\"   Events (init + delta):     {events_size_mb:.2f} MB\")\n",
    "print(f\"   Compression:               {snapshot_to_events_ratio:.1f}×\")\n",
    "print(f\"\")\n",
    "print(f\"   Events breakdown:\")\n",
    "print(f\"      Initialization:         {init_events_df.height:,} events (baseline)\")\n",
    "print(f\"      Delta (5 min):          {delta_events_df.height:,} events ({(delta_events_df.height/init_events_df.height)*100:.2f}% changed)\")\n",
    "\n",
    "print(f\"\\nDaily Projection (288 Snapshots)\")\n",
    "daily_json_mb = json_size_mb * 288\n",
    "daily_parquet_mb = naive_daily_snapshots_mb\n",
    "daily_events_mb = daily_event_storage_mb\n",
    "\n",
    "print(f\"   288 JSON files:            {daily_json_mb:,.0f} MB/day\")\n",
    "print(f\"   288 Parquet files:         {daily_parquet_mb:,.0f} MB/day ({daily_json_mb/daily_parquet_mb:.1f}× vs JSON)\")\n",
    "print(f\"   Event sourcing:            {daily_events_mb:,.0f} MB/day ({daily_parquet_mb/daily_events_mb:.1f}× vs Parquet)\")\n",
    "print(f\"\")\n",
    "print(f\"   Total compression:         {daily_json_mb/daily_events_mb:.1f}× (JSON 28965MB → Events 15MB)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
